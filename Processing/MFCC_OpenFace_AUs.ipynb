{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9be97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from decord import VideoReader, cpu\n",
    "from moviepy.editor import AudioFileClip\n",
    "from scipy.io import wavfile\n",
    "from scipy.fftpack import dct\n",
    "from PIL import Image\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# CONFIGURATION\n",
    "input_size = 224\n",
    "num_frame = 8\n",
    "sampling_rate = 6\n",
    "fps = 25  # frames per second for output video\n",
    "\n",
    "OPENFACE_DIR = r\"D:\\OpenFace_2.2.0_win_x64\\OpenFace_2.2.0_win_x64\"\n",
    "OPENFACE_PATH = os.path.join(OPENFACE_DIR, \"FeatureExtraction.exe\")\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"Sanitize filename to remove special characters and limit length.\"\"\"\n",
    "    sanitized = re.sub(r'[<>:\"/\\\\|?*]', '', filename)\n",
    "    return sanitized[:50]\n",
    "\n",
    "def read_video(file_path):\n",
    "    \"\"\"Read video file and return formatted frames.\"\"\"\n",
    "    vr = VideoReader(file_path, ctx=cpu(0))\n",
    "    frames = vr.get_batch(range(len(vr))).asnumpy()\n",
    "    return format_frames(frames, (input_size, input_size))\n",
    "\n",
    "def format_frames(frame, output_size):\n",
    "    \"\"\"Resize and format video frames.\"\"\"\n",
    "    frame = tf.image.convert_image_dtype(frame, tf.uint8)\n",
    "    frame = tf.image.resize(frame, output_size)\n",
    "    return frame\n",
    "\n",
    "def uniform_temporal_subsample(x, num_samples, clip_idx, total_clips, frame_rate=1, temporal_dim=0):\n",
    "    \"\"\"Uniformly sample frames from video.\"\"\"\n",
    "    t = tf.shape(x)[temporal_dim]\n",
    "    max_offset = t - num_samples * frame_rate\n",
    "    if max_offset < 0:\n",
    "        raise ValueError(\"Video too short for requested clip sampling.\")\n",
    "    step = max_offset // total_clips\n",
    "    offset = clip_idx * step\n",
    "    indices = tf.linspace(\n",
    "        tf.cast(offset, tf.float32),\n",
    "        tf.cast(offset + (num_samples - 1) * frame_rate, tf.float32),\n",
    "        num_samples\n",
    "    )\n",
    "    indices = tf.clip_by_value(indices, 0, tf.cast(t - 1, tf.float32))\n",
    "    indices = tf.cast(tf.round(indices), tf.int32)\n",
    "    return tf.gather(x, indices, axis=temporal_dim)\n",
    "\n",
    "def clip_generator(image, num_frames=8, frame_rate=1, num_clips=1, crop_size=224):\n",
    "    \"\"\"Generate video clips from frames.\"\"\"\n",
    "    clips_list = []\n",
    "    for i in range(num_clips):\n",
    "        frame = uniform_temporal_subsample(\n",
    "            image, num_frames, i, num_clips, frame_rate=frame_rate, temporal_dim=0\n",
    "        )\n",
    "        clips_list.append(frame)\n",
    "    video = tf.stack(clips_list)\n",
    "    video = tf.reshape(video, [num_clips * num_frames, crop_size, crop_size, 3])\n",
    "    return video\n",
    "\n",
    "def normalize_audio(signal):\n",
    "    \"\"\"Normalize audio signal to [-1, 1] range.\"\"\"\n",
    "    return signal / np.max(np.abs(signal))\n",
    "\n",
    "def extract_audio(video_path):\n",
    "    \"\"\"Extract audio from video file and return normalized signal and sample rate.\"\"\"\n",
    "    with AudioFileClip(video_path) as audio_clip:\n",
    "        audio_path = \"temp_audio.wav\"\n",
    "        audio_clip.write_audiofile(audio_path, verbose=False, logger=None)\n",
    "        sample_rate, signal = wavfile.read(audio_path)\n",
    "        os.remove(audio_path)\n",
    "    return normalize_audio(signal), sample_rate\n",
    "\n",
    "def MFCC(signal, samplerate, num_ceps=13, nfilt=26, NFFT=512):\n",
    "    \"\"\"Calculate MFCC features from audio signal.\"\"\"\n",
    "    from scipy.signal import hamming\n",
    "    \n",
    "    # Pre-emphasis\n",
    "    pre_emphasis = 0.97\n",
    "    emphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n",
    "\n",
    "    # Framing\n",
    "    frame_size = 0.025\n",
    "    frame_stride = 0.01\n",
    "    frame_length = int(round(frame_size * samplerate))\n",
    "    frame_step = int(round(frame_stride * samplerate))\n",
    "\n",
    "    signal_length = len(emphasized_signal)\n",
    "    num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))\n",
    "\n",
    "    pad_signal_length = num_frames * frame_step + frame_length\n",
    "    z = np.zeros((pad_signal_length - signal_length))\n",
    "    pad_signal = np.append(emphasized_signal, z)\n",
    "\n",
    "    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + \\\n",
    "              np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "    frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
    "    \n",
    "    # Windowing\n",
    "    frames *= hamming(frame_length)\n",
    "\n",
    "    # Fourier Transform and Power Spectrum\n",
    "    mag_frames = np.absolute(np.fft.rfft(frames, NFFT))\n",
    "    pow_frames = ((1.0 / NFFT) * (mag_frames ** 2))\n",
    "\n",
    "    # Mel Filterbank\n",
    "    low_freq_mel = 0\n",
    "    high_freq_mel = 2595 * np.log10(1 + (samplerate / 2) / 700)\n",
    "    mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)\n",
    "    hz_points = (700 * (10**(mel_points / 2595) - 1))\n",
    "    bin = np.floor((NFFT + 1) * hz_points / samplerate).astype(int)\n",
    "\n",
    "    fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\n",
    "    for m in range(1, nfilt + 1):\n",
    "        f_m_minus, f_m, f_m_plus = bin[m-1], bin[m], bin[m+1]\n",
    "        for k in range(f_m_minus, f_m):\n",
    "            fbank[m-1, k] = (k - bin[m-1]) / (bin[m] - bin[m-1])\n",
    "        for k in range(f_m, f_m_plus):\n",
    "            fbank[m-1, k] = (bin[m+1] - k) / (bin[m+1] - bin[m])\n",
    "\n",
    "    filter_banks = np.dot(pow_frames, fbank.T)\n",
    "    filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)\n",
    "    filter_banks = 20 * np.log10(filter_banks)\n",
    "\n",
    "    # MFCC\n",
    "    mfcc = dct(filter_banks, type=2, axis=1, norm='ortho')[:, :num_ceps]\n",
    "    return mfcc\n",
    "\n",
    "def save_mfcc_as_image(mfcc, output_path, video_name):\n",
    "    \"\"\"Save MFCC features as an image.\"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(mfcc.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.title('MFCC Features')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_file = os.path.join(output_path, f\"{os.path.splitext(video_name)[0]}_mfcc.png\")\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    return output_file\n",
    "\n",
    "def extract_openface_features(video_path, output_dir):\n",
    "    \"\"\"Extract facial features using OpenFace.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    video_name = os.path.basename(video_path)\n",
    "    csv_path = os.path.join(output_dir, f\"{os.path.splitext(video_name)[0]}.csv\")\n",
    "    \n",
    "    command = [\n",
    "        OPENFACE_PATH,\n",
    "        \"-f\", video_path,\n",
    "        \"-out_dir\", output_dir,\n",
    "        \"-of\", os.path.splitext(video_name)[0],\n",
    "        \"-aus\", \"-2Dfp\", \"-pose\", \"-gaze\", \"-verbose\"\n",
    "    ]\n",
    "    \n",
    "    env = os.environ.copy()\n",
    "    env[\"PATH\"] = OPENFACE_DIR + os.pathsep + env.get(\"PATH\", \"\")\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(command, check=True, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"OpenFace failed on {video_name}: {e.stderr.decode('utf-8') if e.stderr else 'Unknown error'}\")\n",
    "        return None\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"OpenFace output not found: {csv_path}\")\n",
    "        return None\n",
    "    \n",
    "    return csv_path\n",
    "\n",
    "def create_video_with_aus(video_path, au_data, output_path, num_frames=8):\n",
    "    \"\"\"Create a video with AU information overlaid on frames.\"\"\"\n",
    "    # Read the original video\n",
    "    vr = VideoReader(video_path, ctx=cpu(0))\n",
    "    total_frames = len(vr)\n",
    "    \n",
    "    # Select evenly spaced frames\n",
    "    frame_indices = np.linspace(0, total_frames-1, num=num_frames, dtype=int)\n",
    "    frames = vr.get_batch(frame_indices).asnumpy()\n",
    "    \n",
    "    # Prepare video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (input_size, input_size))\n",
    "    \n",
    "    # Match AU data with frames\n",
    "    au_data['frame_index'] = au_data['frame'].apply(lambda x: x-1)  # OpenFace frames are 1-based\n",
    "    \n",
    "    for i, frame_idx in enumerate(frame_indices):\n",
    "        frame = frames[i]\n",
    "        frame = cv2.resize(frame, (input_size, input_size))\n",
    "        \n",
    "        # Get AU data for this frame\n",
    "        frame_au = au_data[au_data['frame_index'] == frame_idx]\n",
    "        \n",
    "        if not frame_au.empty:\n",
    "            # Draw AU information on frame\n",
    "            au_values = []\n",
    "            for col in frame_au.columns:\n",
    "                if col.startswith('AU') and '_c' in col and frame_au[col].values[0] > 0:\n",
    "                    au_name = col.split('_')[0]\n",
    "                    au_intensity = frame_au[f\"{au_name}_r\"].values[0]\n",
    "                    au_values.append(f\"{au_name}: {au_intensity:.2f}\")\n",
    "            \n",
    "            # Add text to frame\n",
    "            y_offset = 30\n",
    "            cv2.putText(frame, f\"Frame: {frame_idx+1}/{total_frames}\", (10, y_offset), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "            \n",
    "            for j, au_text in enumerate(au_values):\n",
    "                y_offset += 25\n",
    "                cv2.putText(frame, au_text, (10, y_offset), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "        \n",
    "        # Write frame to output video\n",
    "        out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    out.release()\n",
    "    return output_path\n",
    "\n",
    "def process_video(video_path, output_dir):\n",
    "    \"\"\"Process a single video file to extract MFCC and video with AUs.\"\"\"\n",
    "    video_name = os.path.basename(video_path)\n",
    "    print(f\"Processing {video_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create temporary directory\n",
    "    temp_dir = os.path.join(output_dir, \"temp\")\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # 1. Extract audio and compute MFCC\n",
    "        audio_signal, sample_rate = extract_audio(video_path)\n",
    "        mfcc = MFCC(audio_signal, sample_rate)\n",
    "        mfcc_image_path = save_mfcc_as_image(mfcc, output_dir, video_name)\n",
    "        \n",
    "        # 2. Extract facial features with OpenFace\n",
    "        openface_csv = extract_openface_features(video_path, temp_dir)\n",
    "        if openface_csv is None:\n",
    "            raise Exception(\"OpenFace feature extraction failed\")\n",
    "        \n",
    "        # Read AU data\n",
    "        au_data = pd.read_csv(openface_csv)\n",
    "        \n",
    "        # 3. Create video with AU information\n",
    "        output_video_path = os.path.join(output_dir, f\"{os.path.splitext(video_name)[0]}_aus.mp4\")\n",
    "        create_video_with_aus(video_path, au_data, output_video_path, num_frames=8)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Successfully processed {video_name} in {elapsed:.2f} seconds.\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {video_name}: {str(e)}\")\n",
    "        return False\n",
    "        \n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        if os.path.exists(temp_dir):\n",
    "            for attempt in range(5):\n",
    "                try:\n",
    "                    shutil.rmtree(temp_dir)\n",
    "                    break\n",
    "                except PermissionError:\n",
    "                    time.sleep(1)\n",
    "            else:\n",
    "                print(f\"Warning: Could not delete temporary directory {temp_dir}\")\n",
    "\n",
    "def process_videos(input_path, output_path):\n",
    "    \"\"\"Process all videos in the input directory.\"\"\"\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    video_files = [f for f in os.listdir(input_path) if f.lower().endswith(('.avi', '.mp4'))]\n",
    "    total_videos = len(video_files)\n",
    "    processed_count = 0\n",
    "    \n",
    "    print(f\"Starting processing of {total_videos} videos...\")\n",
    "    \n",
    "    for i, video_file in enumerate(video_files, 1):\n",
    "        video_path = os.path.join(input_path, video_file)\n",
    "        if process_video(video_path, output_path):\n",
    "            processed_count += 1\n",
    "        \n",
    "        print(f\"Progress: {i}/{total_videos} videos processed\")\n",
    "    \n",
    "    print(f\"\\nFinished processing. Successfully processed {processed_count} of {total_videos} videos.\")\n",
    "    return processed_count\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #input_path = r\"D:\\eNTERFACE\\Enterface_videos\"\n",
    "    #output_path = r\"D:\\eNTERFACE\\Enterface_videoss\"\n",
    "    \n",
    "    #processed_count = process_videos(input_path, output_path)\n",
    "    #print(f\"\\nProcessing complete. {processed_count} videos were successfully processed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
